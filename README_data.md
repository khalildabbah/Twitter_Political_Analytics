## Data Collection Layer for Twitter/X Political Analytics Dashboard

This directory describes the **offline Python data collection pipeline** that feeds
the Next.js dashboard. The dashboard UI is not touched by this script – it only
writes JSON/CSV files under `data/` at the project root.

### 1. Python environment setup

From the project root (`Twitter_Political_Analytics/`), create and activate a
virtual environment.

On **Windows (PowerShell)**:

```bash
python -m venv .venv
.venv\Scripts\Activate.ps1
```

On **macOS / Linux**:

```bash
python -m venv .venv
source .venv/bin/activate
```

Upgrade `pip` (recommended):

```bash
python -m pip install --upgrade pip
```

### 2. Install dependencies

Inside the virtualenv, install the required Python packages:

```bash
pip install snscrape pandas
```

> Note: `snscrape` is a Python library for scraping Twitter/X and other social
> networks without using the official API.

### 3. Running the scraper

From the project root, run:

```bash
python backend/scraping/scrape_tweets.py
```

What this script does:

- Ensures a `data/` directory exists at the project root (and a `data/raw//`
  subfolder for future extensions).
- Scrapes recent tweets (up to a per-account cap and 30 days back) from the
  accounts listed in the `ACCOUNTS` list inside
  `backend/scraping/scrape_tweets.py`.
- Collects per-tweet fields such as:
  - tweet id, URL, username, display name
  - group (Hadash-Ta'al, Ra'am, Islamic/Independent, Activist)
  - human-readable label
  - full text, language, creation datetime (ISO string)
  - like / retweet / reply / quote counts
  - computed `virality_score = likes + retweets + replies + quotes`
- Writes two files:
  - `data/all_tweets.csv`
  - `data/all_tweets.json` (pretty-printed)

The script logs simple progress messages like:

```text
Scraping @AyOdeh (Hadash-Ta'al)...
Finished @AyOdeh (Hadash-Ta'al) — collected XXX tweets
```

Errors for individual accounts (e.g. TODO placeholders, suspended users) are
logged as warnings and **do not** stop the whole run.

### 4. Topic and Narrative Analysis (OpenAI)

To extract topics and recurring narratives from tweets using OpenAI:

1. Install the OpenAI package:
   ```bash
   pip install openai
   ```

2. Set your OpenAI API key as an environment variable:
   ```bash
   # Windows (PowerShell)
   $env:OPENAI_API_KEY="your-api-key-here"
   
   # macOS / Linux
   export OPENAI_API_KEY="your-api-key-here"
   ```

3. Run the analysis script:
   ```bash
   python backend/analyze_topics_and_narratives.py
   ```

This script:
- Reads `data/all_tweets.json`
- Groups tweets by account
- Analyzes the most recent 20-30 tweets per account using `gpt-4o-mini`
- Extracts top 5 topics and 2-3 recurring narratives for each account
- Writes results to `data/topics_and_narratives.json`

The analysis uses a neutral, descriptive approach without sentiment analysis.
Quantitative metrics (likes, retweets, etc.) are handled separately in the dashboard.

### 5. How this connects to the dashboard

The Next.js dashboard reads from JSON files generated by these Python scripts:

- `data/all_tweets.json` - All tweet data with engagement metrics
- `data/topics_and_narratives.json` - Qualitative topic and narrative analysis

The `backend/` folder is intentionally self-contained so you can iterate on the
Python analysis (using pandas, OpenAI, offline models, etc.) without changing the UI
code.

