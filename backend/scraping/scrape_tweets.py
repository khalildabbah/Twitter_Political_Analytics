"""
Offline data collection script for the Twitter/X Political Analytics Dashboard.

This script uses snscrape to fetch recent tweets from a curated list of
political accounts and saves them under the project-level `data/` directory.

The Next.js frontend currently uses mock data, but in the future it can read
from `data/all_tweets.json` (and/or `data/all_tweets.csv`) generated by this
script instead.
"""

from __future__ import annotations

import json
import logging
from dataclasses import asdict, dataclass
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Any, Dict, List

import pandas as pd
from snscrape.modules.twitter import TwitterUserScraper


@dataclass
class Account:
    handle: str
    group: str
    label: str


# List of accounts to scrape.
# Each entry has:
# - handle: Twitter/X username without the leading '@'
# - group: logical grouping used in the dashboard
# - label: human-readable display label
ACCOUNTS: List[Dict[str, str]] = [
    # =========================
    # Hadash–Ta'al
    # =========================
    {"handle": "AyOdeh", "group": "Hadash-Ta'al", "label": "Ayman Odeh"},
    {"handle": "Ahmad_tibi", "group": "Hadash-Ta'al", "label": "Ahmad Tibi"},
    {"handle": "AidaTuma", "group": "Hadash-Ta'al", "label": "Aida Touma-Sliman"},
    {"handle": "ofercass", "group": "Hadash-Ta'al", "label": "Ofer Cassif"},
    {"handle": "DrJabareen", "group": "Hadash-Ta'al", "label": "Yousef Jabareen"},

    # =========================
    # Ra'am
    # =========================
    {"handle": "mnsorabbas", "group": "Ra'am", "label": "Mansour Abbas"},
    {"handle": "Waleedt68", "group": "Ra'am", "label": "Waleed Taha"},
    {"handle": "WalidAlhwashla", "group": "Ra'am", "label": "Walid Al-Hawashla"},
    {"handle": "IbrahimSarsour", "group": "Ra'am", "label": "Ibrahim Sarsour"},
    {"handle": "TalalAlkrinawi", "group": "Ra'am", "label": "Talal Al-Qrenawi"},

    # =========================
    # Islamic Movement / Independents
    # =========================
    {"handle": "RaedSalah", "group": "Islamic/Independent", "label": "Sheikh Raed Salah"},
    {"handle": "MasarwaAmna", "group": "Islamic/Independent", "label": "Amna Masarwa"},
    {"handle": "HassanJabareen", "group": "Islamic/Independent", "label": "Hassan Jabareen"},
    {"handle": "SuhailDiab", "group": "Islamic/Independent", "label": "Suhail Diab"},
    {"handle": "KamalKhatib1", "group": "Islamic/Independent", "label": "Kamal Khatib"},

    # =========================
    # Activists (outside parties)
    # =========================
    {"handle": "SSinijlawi", "group": "Activist", "label": "Sawsan Sinijlawi"},
    {"handle": "HowidyHamza", "group": "Activist", "label": "Hamza Howidy"},
    {"handle": "YousefMunayyer", "group": "Activist", "label": "Yousef Munayyer"},
    {"handle": "LeanneMohamad", "group": "Activist", "label": "Leanne Mohamad"},
    {"handle": "HaneenZoabi", "group": "Activist", "label": "Haneen Zoabi"},
]


def _project_root() -> Path:
    """Return the project root (assumed to be two levels up from this file)."""
    return Path(__file__).resolve().parents[2]


def _ensure_data_dirs() -> Path:
    """
    Ensure that `data/` (and `data/raw/`) exist at the project root.

    Returns
    -------
    Path
        Path to the `data/` directory.
    """
    root = _project_root()
    data_dir = root / "data"
    raw_dir = data_dir / "raw"
    data_dir.mkdir(parents=True, exist_ok=True)
    raw_dir.mkdir(parents=True, exist_ok=True)
    return data_dir


def scrape_account(account: Dict[str, str], max_tweets_per_user: int = 50 , days_back: int = 30) -> List[Dict[str, Any]]:
    """
    Scrape recent tweets for a single account.

    Parameters
    ----------
    account : dict
        Dictionary with at least `handle`, `group`, and `label`.
    max_tweets_per_user : int
        Maximum number of tweets to collect per user.
    days_back : int
        Stop when tweets are older than this many days.
    """
    handle = account["handle"]
    group = account["group"]
    label = account["label"]

    cutoff = datetime.now(timezone.utc) - timedelta(days=days_back)
    tweets_data: List[Dict[str, Any]] = []

    logging.info("Scraping @%s (%s)...", handle, group)

    scraper = TwitterUserScraper(handle)
    for i, tweet in enumerate(scraper.get_items(), start=1):
        # Stop if we reached the per-user limit
        if i > max_tweets_per_user:
            break

        # Stop if tweet is older than the cutoff
        if tweet.date is not None and tweet.date < cutoff:
            break

        # Basic engagement metrics (snscrape names may vary slightly between versions)
        likes = getattr(tweet, "likeCount", 0) or 0
        retweets = getattr(tweet, "retweetCount", 0) or 0
        replies = getattr(tweet, "replyCount", 0) or 0
        quotes = getattr(tweet, "quoteCount", 0) or 0

        virality_score = likes + retweets + replies + quotes

        tweets_data.append(
            {
                "tweet_id": str(getattr(tweet, "id", "")),
                "url": getattr(tweet, "url", ""),
                "username": getattr(tweet, "user", {}).username
                if getattr(tweet, "user", None)
                else handle,
                "display_name": getattr(tweet, "user", {}).displayname
                if getattr(tweet, "user", None)
                else label,
                "group": group,
                "label": label,
                "full_text": getattr(tweet, "content", ""),
                "language": getattr(tweet, "lang", None),
                "created_at": tweet.date.isoformat() if getattr(tweet, "date", None) else None,
                "like_count": likes,
                "retweet_count": retweets,
                "reply_count": replies,
                "quote_count": quotes,
                "virality_score": virality_score,
            }
        )

    logging.info("Finished @%s (%s) — collected %d tweets", handle, group, len(tweets_data))
    return tweets_data


def scrape_all_accounts(max_tweets_per_user: int = 300, days_back: int = 30) -> List[Dict[str, Any]]:
    """Scrape tweets for all accounts defined in ACCOUNTS."""
    all_tweets: List[Dict[str, Any]] = []

    for account in ACCOUNTS:
        handle = account["handle"]
        group = account["group"]
        label = account["label"]
        try:
            logging.info("=== Starting account @%s (%s) ===", handle, group)
            account_tweets = scrape_account(account, max_tweets_per_user=max_tweets_per_user, days_back=days_back)
            all_tweets.extend(account_tweets)
        except Exception as exc:  # noqa: BLE001
            logging.warning(
                "Failed to scrape @%s (%s - %s): %s",
                handle,
                group,
                label,
                exc,
                exc_info=True,
            )

    logging.info("Scraping complete. Total tweets collected: %d", len(all_tweets))
    return all_tweets


def main() -> None:
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )

    data_dir = _ensure_data_dirs()
    logging.info("Using data directory: %s", data_dir)

    all_tweets = scrape_all_accounts(max_tweets_per_user=300, days_back=30)

    if not all_tweets:
        logging.warning("No tweets collected — check accounts or network connectivity.")
        return

    # Save as CSV
    df = pd.DataFrame(all_tweets)
    csv_path = data_dir / "all_tweets.csv"
    df.to_csv(csv_path, index=False)
    logging.info("Wrote CSV to %s", csv_path)

    # Save as JSON
    json_path = data_dir / "all_tweets.json"
    with json_path.open("w", encoding="utf-8") as f:
        json.dump(all_tweets, f, ensure_ascii=False, indent=2)
    logging.info("Wrote JSON to %s", json_path)


if __name__ == "__main__":
    main()

